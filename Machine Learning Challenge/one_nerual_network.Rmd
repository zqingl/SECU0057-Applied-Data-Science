---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
library(tensorflow)
library(keras)
library(tidyverse)
library(quanteda)
library(syuzhet)
```

```{r}
# Read training data and submitting data
train_data <- read.csv("training_data.csv")
test_data <- read.csv("submission_data.csv")

# Set training indices and validation indices
train_indices <- 1:2000

# Set an extra column to ensure that the data is correctly arranged after merged together
train_data$ml_id <- c(1:2000)
test_data$ml_id <- c(2001:3000)

# Merge training and submission dataset and order
raw_data <-rbind(train_data,test_data)
raw_data <- raw_data[order(raw_data$ml_id),]

# Preprocessing raw data to remove special characters
raw_data <- raw_data %>% 
   mutate(text = iconv(text, from = "UTF-8", to = "ASCII",sub=''))%>%
  mutate(text = str_remove_all(text, "<br />")) %>%
mutate(text = str_replace_all(text, "[^[:alnum:]]"," "))
```

```{r}
# Tokenize text data and return an dfm object.
tokens = raw_data$text %>% 
tokens(remove_punct = T,remove_symbols=T,remove_numbers=T,remove_url=T,
         split_hyphens = T)%>%
  tokens_remove(stopwords(language = "en")) %>%
  dfm()
```

```{r}
# Calculate an inverse loge document frequency matrix for all unique tokens. 
doc_freq <- docfreq(
  tokens,
  scheme =  "inverse",
  base = 2.718281,
  smoothing = 0,
  k = 0,
  threshold = 0
)

# Convert to dataframe for the convenience of calculation.
doc_freq_mt <- as.data.frame(doc_freq)

# This step will weight the feature frequencies of a dfm. It will recode all non-zero counts as 1. We only care if specific token appears in a document all not, the count doesn't matter.
ml_tokens <- dfm_weight(tokens,scheme='boolean')
ml_tokens <- as.data.frame(ml_tokens)
ml_tokens <- ml_tokens[,-1]

# Finally, use sweep function to calculate the weighted inverse loge document frequency matrix for the text column. This matrix reflects the rareness of the vocabularies adopted by each comment.
ml_tokens_cal<- sweep(ml_tokens, MARGIN=2, doc_freq_mt[['doc_freq']], `*`)
head(ml_tokens_cal)
```
```{r}
# Using the above mentioned matrix, we can build three new featurs: count of rare word, sum rare score and average rare score. However, we will only use average_rare score as a new feature.
ml_tokens_cal$count <- rowSums(ml_tokens_cal!=0)
ml_tokens_cal$sum_rare_score <- rowSums(ml_tokens_cal, na.rm = TRUE)
ml_tokens_cal$avg_rare_score <- (ml_tokens_cal$sum_rare_score/ml_tokens_cal$count)
```

```{r}
# It's obvious that the pertubated comments tend to use rare words compared to the original comments. However, it seems that there's no major difference in the sum rare score. We will only use avg rare score as a new feature.
boxplot(ml_tokens_cal$avg_rare_score ~ raw_data$label,names=c("Pertubated","Original"),ylab="Avg Rare Score")
boxplot(ml_tokens_cal$sum_rare_score ~ raw_data$label,names=c("Pertubated","Original"),ylab="Sum Rare Score")
```
```{r}
# Leave only avg rare score with the inverse loge weighted document frequency matrix as input features.
ml_tokens_cal <-ml_tokens_cal %>% select(-count,-sum_rare_score)
```

```{r}
ml_tokens_std<-scale(ml_tokens_cal)
head(ml_tokens_std)
```

```{r}
# Finally, we will covert the dataframe into matrix that is accepted Keras.
input_features <- data.matrix(ml_tokens_cal)
```

```{r}
t(apply(input_features, 1, function(x)(x-min(x))/(max(x)-min(x))))
```

```{r}
head(input_featurures)
```

```{r}
# Split train and test data
x_train <- input_features[train_indices,]
y_train <- as.numeric(raw_data[train_indices,]$label)
x_test <- input_features[-train_indices,]
y_test <- as.numeric(raw_data[-train_indices,]$label)

# Split train and validation data
val_indices <- 1:200
partial_x_train <- x_train[-val_indices,]
partial_y_train <- y_train[-val_indices]
x_val <- x_train[val_indices,]
y_val <- y_train[val_indices]
dim(x_train)
```

```{r}
model <- keras_model_sequential() %>%
layer_dense(units = 128, input_shape = c(32191),activation = "relu",regularizer_l1_l2(l1=0.05,l2=0.05)) %>%
  layer_dropout(0.5) %>%
 layer_dense(units = 64, activation = "relu",regularizer_l1_l2(l1=0.05,l2=0.05)) %>%
  layer_dropout(0.5) %>%
  layer_dense(units = 32, activation = "relu",regularizer_l1_l2(l1=0.05,l2=0.05)) %>%
  
  
  
 layer_dense(units = 1, activation = "sigmoid")
summary(model)
```

```{r}
model %>% compile(
 optimizer = 'adam',
 loss = "binary_crossentropy",
 metrics = c("accuracy")
)

model %>% fit(
 x_train,
 y_train,
 epochs = 10,
 batch_size = 200,
 validation_data = list(x_test, y_test)
)
```

```{r}
model %>% evaluate(x_test,y_test)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
